{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "83d3e779-d936-424c-8c1e-635d680f625d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1f137bc4-2d5b-402d-aff3-f8a225176fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"sms_spam.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cfddd94-5e7b-4aea-bc52-44b0d6b743a7",
   "metadata": {},
   "source": [
    "**Proccessing the Data before training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "120e4099-48df-4a2b-9476-1c92e53bb6e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.13406317300789664\n",
      "0.8659368269921034\n"
     ]
    }
   ],
   "source": [
    "#create an array that is 0 if ham and 1 if spam\n",
    "labels = (df[\"label\"]==\"spam\").astype(int)\n",
    "\n",
    "stopwords = [\"the\", \"and\", \"is\", \"in\", \"to\", \"a\", \"of\", \"i\", \"you\", \"u\", \"me\", \"it\", \"for\", \"your\", \"my\"]\n",
    "#lower to lowercase a string, str to apply it to all strings in the column\n",
    "X_texts = df[\"message\"].str.lower()\n",
    "\n",
    "#replace any characters that are not a-z or 0-9\n",
    "X_texts = X_texts.replace(r'\\W+', \" \", regex=True)\n",
    "\n",
    "#split words and remove any stopwords\n",
    "X_texts  = [text.split() for text in X_texts]\n",
    "X_texts = [[w for w in text if w not in stopwords] for text in X_texts]\n",
    "\n",
    "#create a dictionary that counts the instances of each word in X_texts\n",
    "vocab = Counter()\n",
    "[vocab.update(text) for text in X_texts]\n",
    "\n",
    "#create a dict of the most common words as keys and index as value\n",
    "vocab = {word:idx for idx, (word,__) in enumerate(vocab.most_common(3000))}\n",
    "\n",
    "#create an array of vectors to show how many times a word from vocab appears in each message\n",
    "X_data = [[0]*len(vocab) for i in range(len(X_texts))] ##array of vectors of 0s as initialization\n",
    "for i, text in enumerate(X_texts):\n",
    "    for word in text: \n",
    "        if(word in vocab):\n",
    "            X_data[i][vocab[word]]=+1\n",
    "print(len(df[df[\"label\"]==\"spam\"])/len(df))\n",
    "print(len(df[df[\"label\"]==\"ham\"])/len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "ee4815a4-3ea8-4bc7-882a-e16a0a4ddd87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1/(1+np.exp(-z))\n",
    "\n",
    "def init_weights(n_features):\n",
    "    return np.zeros(n_features), 0.0\n",
    "\n",
    "def predict(X, W, b, threshold=0.5):\n",
    "    linear = np.dot(X, W) +b\n",
    "    pred = sigmoid(linear)\n",
    "    return(pred >= threshold).astype(int)\n",
    "\n",
    "def train(X_train, Y_train, lr=0.01, epochs=1000):\n",
    "    W,b = init_weights(len(vocab))\n",
    "    \n",
    "    for _ in range(epochs):\n",
    "        Y_pred = sigmoid(np.dot(X_train,W) + b)\n",
    "        \n",
    "        dw = np.dot(X_train.T, (Y_pred - Y_train)) / len(X_train)\n",
    "        db = np.sum(Y_pred-Y_train) / len(X_train)\n",
    "        \n",
    "        W -= lr * dw\n",
    "        b -= lr * db\n",
    "    return W, b\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "0ecdd937-33ae-4fc1-ac0e-ac1fa3022741",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data = np.array(X_data)\n",
    "Y_data = np.array(labels)\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_data, Y_data, stratify=Y_data, test_size = 0.2)\n",
    "\n",
    "W, b = train(X_train, Y_train, lr=0.15, epochs=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "eba61a37-3f9c-418f-a26c-d28372a90c67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.57671485 -0.46831001  0.0375414  ... -0.00337246 -0.00876347\n",
      " -0.00700808] -2.947637797746006\n"
     ]
    }
   ],
   "source": [
    "Y_test_predict = predict(X_test, W, b, threshold=0.5)\n",
    "print(W, b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "b83f80c9-a364-45e1-a799-4abde84718bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 1.0\n",
      "Recall: 0.8187919463087249\n",
      "F1 Score: 0.900369003690037\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "print(\"Precision:\", precision_score(Y_test, Y_test_predict))\n",
    "print(\"Recall:\", recall_score(Y_test, Y_test_predict))\n",
    "print(\"F1 Score:\", f1_score(Y_test, Y_test_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "823181c3-4c8f-41e8-953e-d216b274c57e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[966   0]\n",
      " [ 79  70]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.924     1.000     0.961       966\n",
      "           1      1.000     0.470     0.639       149\n",
      "\n",
      "    accuracy                          0.929      1115\n",
      "   macro avg      0.962     0.735     0.800      1115\n",
      "weighted avg      0.935     0.929     0.918      1115\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "print(confusion_matrix(Y_test, Y_test_predict))\n",
    "print(classification_report(Y_test, Y_test_predict, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "084c9e07-04f0-4b26-81dc-5ef06f794615",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: (array([0]), array([1115], dtype=int64))\n",
      "Actual: (array([0, 1]), array([966, 149], dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "y_pred = predict(X_test, W, b)\n",
    "print(\"Predictions:\", np.unique(y_pred, return_counts=True))\n",
    "print(\"Actual:\", np.unique(Y_test, return_counts=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "6eb7ef60-5999-493a-9667-cc814939078f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Threshold: 0.5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.87      1.00      0.93       966\n",
      "        spam       0.00      0.00      0.00       149\n",
      "\n",
      "    accuracy                           0.87      1115\n",
      "   macro avg       0.43      0.50      0.46      1115\n",
      "weighted avg       0.75      0.87      0.80      1115\n",
      "\n",
      "\n",
      "Threshold: 0.4\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.87      1.00      0.93       966\n",
      "        spam       1.00      0.06      0.11       149\n",
      "\n",
      "    accuracy                           0.87      1115\n",
      "   macro avg       0.94      0.53      0.52      1115\n",
      "weighted avg       0.89      0.87      0.82      1115\n",
      "\n",
      "\n",
      "Threshold: 0.3\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.92      1.00      0.96       966\n",
      "        spam       1.00      0.47      0.64       149\n",
      "\n",
      "    accuracy                           0.93      1115\n",
      "   macro avg       0.96      0.73      0.80      1115\n",
      "weighted avg       0.93      0.93      0.92      1115\n",
      "\n",
      "\n",
      "Threshold: 0.2\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.99      0.91      0.95       966\n",
      "        spam       0.61      0.94      0.74       149\n",
      "\n",
      "    accuracy                           0.91      1115\n",
      "   macro avg       0.80      0.92      0.84      1115\n",
      "weighted avg       0.94      0.91      0.92      1115\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\samat\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\samat\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\samat\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "for t in [0.5, 0.4, 0.3, 0.2]:\n",
    "    y_pred = predict(X_test, W, b, threshold=t)\n",
    "    print(f\"\\nThreshold: {t}\")\n",
    "    print(classification_report(Y_test, y_pred, target_names=[\"ham\", \"spam\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9799fa75-a304-4bc9-ae7f-79ca0e397fb1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
